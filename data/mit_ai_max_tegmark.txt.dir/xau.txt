max tegmark:
open ended goal and the machine is very intelligent, it'll break that down into a bunch of sub goals. And um one of those goals will almost always be self preservation because if it breaks or dies in the process, it's not going to accomplish the goal, right? Like suppose you just build a little, you have a little robot and you tell it to go down the star market here and, and, and get you some food, make you cook an Italian dinner, you know, and then someone mugs it and tries to break it on the way that robot has an incentive to, to not get destroyed and defend itself or running away because otherwise it's gonna fail and cooking your dinner, it's not afraid of death, but it really wants to complete the dinner, cooking gold. So it will have a self preservation instinct to continue being a functional agent somehow. And, and, and similarly, if you give any kind of more ambitious goal to an A G I, it's very likely they want to acquire more resources so we can do that better. And it's exactly from those sort of sub