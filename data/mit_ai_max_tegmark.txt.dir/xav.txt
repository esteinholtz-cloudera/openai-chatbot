max tegmark:
 goals that we might not have intended that that some of the concerns about A G I safety come you give it some goal which seems completely harmless. And then before you realize it, it's also trying to do these other things that you didn't want to do and it's maybe smarter than us. So, so, and let me pause just because, um, I am in a very kind of human centric way. See fear of death as a valuable motivator. So you don't think uh so do you think that's an artifact of evolution? So that's the kind of mind space evolution created that we're sort of almost obsessed about self-preservation, some kind of genetic. Well, you don't think that's necessary to be afraid of death. So not just a, a kind of sub goal of self preservation just so you can keep doing the thing, but more fundamentally sort of have the finite thing like this ends for you at some point. Interesting. Do, do I think it's necessary for what precisely for intelligence? But also for consciousness? So for those, for both, do you think really like a finit