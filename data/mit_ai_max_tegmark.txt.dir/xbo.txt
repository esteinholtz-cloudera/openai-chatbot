max tegmark:
d it didn't work out so well for the rhinoceros because we were more intelligent, right? So I think it's just so important that if we ever do build a G I before we unleash anything, we have to make sure that it's, it learns to understand our goals that adopts our goals and it retains those goals. So the cool interesting problem there is being able us as human beings trying to formulate our values. So, you know, you could think of the United States Constitution as a, as a way that people sat down at the time, a bunch of white men, but which is a good example. I should, we should say uh they formulated the goals for this country and a lot of people agree that those goals actually held up pretty well. It's an interesting formulation of values and failed miserably in other ways. So for the value alignment problem and the solution to it, we have to be able to put on paper uh or in, in uh in a program, human values. How difficult do you think that is very, but it's so important, we really have to give it our best a