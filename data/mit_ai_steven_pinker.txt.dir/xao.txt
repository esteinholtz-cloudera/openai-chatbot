steven pinker:
to destroy things. So the analogy is is misleading. So there's two artificial intelligence, you mentioned the first one, I guess the high in power hungry a system that we design ourselves where we give it the goals, goals are, are external to the uh means to attain the goals. Uh If we don't design an artificial intelligence system to maximize uh dominance, then it won't maximize dominance. Uh It's just that we're so familiar with Homo Sapiens where these two traits come bundled together, particularly in men that we are apt to confuse high intelligence with uh a uh a will to power, but that's just an error. Um The other fear is that will be collateral damage that will give uh artificial intelligence uh a goal like make paper clips and it will pursue that goal so brilliantly that before we can stop it, it turns us into paper clips. Uh We'll give it the goal of curing cancer and it will turn us into guinea pigs for lethal experiments or give it the goal of world peace and it, its conception of world peace is no 