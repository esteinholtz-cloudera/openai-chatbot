steven pinker:
people, therefore no fighting. And so it will kill us all. Now, I think these are utterly fanciful. In fact, I think they're actually self defeating. They first of all assume that we're gonna be so brilliant that we can design an artificial intelligence that can cure cancer, but so stupid that we don't specify what we mean by curing cancer in enough detail that it won't kill us in the process. Uh And it assumes that the system will be so smart that it can cure cancer, but so idiotic that it doesn't, can't figure out that what we mean by curing cancer is not killing everyone. So I think that the the collateral damage scenario, the value alignment problem is uh is also based on a misconception. So one of the challenges, of course, we don't know how to build either system currently or are we even close to knowing? Of course, those things can change overnight? But at this time, theorizing about it is very challenging uh in, in either direction. So that that's probably at the core of the problem is without that ab