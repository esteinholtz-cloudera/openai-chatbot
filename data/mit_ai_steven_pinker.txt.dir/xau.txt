steven pinker:
 whole literature sympathizing with the people stuck in these menial mine, deadening uh dangerous jobs if we can eliminate them. This is a fantastic boon to humanity. Now, granted, we, you solve one problem and there's another one, namely, how do we get these people a, a decent income. But if we're smart enough to invent machines that can make beds and, and put away dishes and uh, and, and handle hospital patients. Well, I think we're smart enough to figure out how to redistribute income to apportion some of the vast economic savings to the, the human beings who will no longer be needed to, to make beds. Ok. Sam Harris says that it's obvious that eventually A I will be an existential risk. He's one of the people who says it's obvious we don't know when the claim goes, but eventually it's obvious and because we don't know when we should worry about it now, it's a very interesting argument in my eyes. So how do we think about timescale? How do we think about existential threats when we don't really, we know so 