yoshua bengio:
ct of the way deep neural networks represent the world. What is the, what is in your view is missing? So currently current state of the art Neil nets trained on large quantities of images or texts have some level of understanding of, you know, what explains those data sets. But it's very basic, it's, it's very low level and it's not nearly as robust and abstract and general as our understanding. OK. So that doesn't tell us how to fix things. But I think it encourages us to think about how we can maybe train our neural nets differently. So that they would focus, for example, on causal explanation is something that we don't do currently with neural net training. Also one thing I'll talk about in my talk this afternoon is instead of learning separately from images and videos on one hand and from text on the other hand, we need to do a better job of um jointly learning about language and about the world to which it refers so that you know, both sides can help each other. We need to have good world models in in ou